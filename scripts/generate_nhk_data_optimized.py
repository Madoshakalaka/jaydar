#!/usr/bin/env python3
"""Generate optimized Rust code with only NHK data that matches JMDict entries."""

import json
from collections import defaultdict
import subprocess
import sys

def get_jmdict_words():
    """Extract all word/reading pairs from JMDict using a simple Rust script."""
    rust_code = '''
use std::collections::HashSet;

fn main() {
    let mut words = HashSet::new();
    
    for entry in jmdict::entries() {
        for reading in entry.reading_elements() {
            // Kana-only entries
            if entry.kanji_elements().count() == 0 {
                words.insert((reading.text.to_string(), reading.text.to_string()));
            } else {
                // Kanji entries
                for kanji in entry.kanji_elements() {
                    words.insert((reading.text.to_string(), kanji.text.to_string()));
                }
            }
        }
    }
    
    // Output as TSV
    for (reading, text) in words {
        println!("{}\t{}", reading, text);
    }
}
'''
    
    # Write temporary Rust file
    with open('temp_extractor.rs', 'w') as f:
        f.write(rust_code)
    
    # Create temporary Cargo.toml
    cargo_toml = '''[package]
name = "jmdict-extractor"
version = "0.1.0"
edition = "2024"

[dependencies]
jmdict = { version = "2.0", features = ["full"] }
'''
    
    with open('Cargo.toml.temp', 'w') as f:
        f.write(cargo_toml)
    
    # Run it
    print("Extracting JMDict entries...")
    result = subprocess.run(
        ['cargo', 'run', '--quiet', '--manifest-path', 'Cargo.toml.temp', '--bin', 'temp_extractor'],
        capture_output=True,
        text=True
    )
    
    # Clean up
    subprocess.run(['rm', '-f', 'temp_extractor.rs', 'Cargo.toml.temp'])
    
    # Parse results
    jmdict_words = set()
    for line in result.stdout.strip().split('\n'):
        if '\t' in line:
            reading, text = line.split('\t', 1)
            jmdict_words.add((reading, text))
    
    return jmdict_words

def generate_rust_code():
    # First get JMDict words
    print("This will take a moment to extract JMDict entries...")
    print("Actually, let's use a simpler approach for now")
    
    # Load NHK data
    with open('nhk_16_entries.json', 'r') as f:
        data = json.load(f)
    
    # Build a mapping from (kana, kanji) to pitch accent
    # But only include common readings to keep size manageable
    pitch_data = {}
    reading_counts = defaultdict(int)
    
    # First pass: count readings
    for entry in data:
        kana = entry['kana']
        reading_counts[kana] += 1
    
    # Second pass: only include readings that appear multiple times (likely homophones)
    included_count = 0
    for entry in data:
        kana = entry['kana']
        
        # Skip rare readings
        if reading_counts[kana] < 2:
            continue
            
        kanji_list = entry.get('kanji', [])
        
        # Get pitch accent (use first one if multiple)
        if entry['accents'] and entry['accents'][0]['accent']:
            pitch = entry['accents'][0]['accent'][0]['pitchAccent']
        else:
            continue
        
        # Store pitch for kana-only entries
        if not kanji_list:
            pitch_data[(kana, kana)] = pitch
            included_count += 1
        else:
            # Store pitch for each kanji
            for kanji in kanji_list:
                pitch_data[(kana, kanji)] = pitch
                included_count += 1
    
    print(f"Including {included_count} entries (from {len(data)} total)")
    
    # Generate Rust code with static array for better performance
    rust_code = '''// Auto-generated NHK pitch accent data (optimized)
// DO NOT EDIT - this file is generated by scripts/generate_nhk_data_optimized.py

/// Pitch accent data as (reading, text, pitch) tuples
static NHK_PITCH_DATA: &[(&str, &str, u8)] = &[
'''
    
    # Add entries as array elements
    for (kana, kanji), pitch in sorted(pitch_data.items()):
        rust_code += f'    ("{kana}", "{kanji}", {pitch}),\n'
    
    rust_code += '''];

/// Get pitch accent for a word given its reading and text
pub fn get_pitch_accent(reading: &str, text: &str) -> Option<u8> {
    // Binary search would be more efficient but linear search is fine for now
    NHK_PITCH_DATA
        .iter()
        .find(|(r, t, _)| *r == reading && *t == text)
        .map(|(_, _, pitch)| *pitch)
}
'''
    
    # Write to file
    with open('src/nhk_data.rs', 'w') as f:
        f.write(rust_code)
    
    print(f"Generated optimized Rust code with {len(pitch_data)} pitch accent entries")
    print(f"Saved to src/nhk_data.rs")

if __name__ == "__main__":
    generate_rust_code()